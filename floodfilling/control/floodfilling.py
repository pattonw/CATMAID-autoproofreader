# -*- coding: utf-8 -*-
import datetime
import subprocess
from pathlib import Path
import json
import numpy as np
import time

from django.conf import settings
from django.http import JsonResponse
from django.utils.decorators import method_decorator
from django.db import connection
from django.shortcuts import get_object_or_404

from catmaid.consumers import msg_user
from catmaid.control.volume import get_volume_instance
from catmaid.control.authentication import requires_user_role
from catmaid.models import Message, User, UserRole
from catmaid.control.message import notify_user

from celery.task import task

from rest_framework.views import APIView

from floodfilling.models import (
    FloodfillResult,
    FloodfillConfig,
    ComputeServer,
    FloodfillModel,
)
from floodfilling.control.compute_server import GPUUtilAPI


# The path were server side exported files get stored in
output_path = Path(settings.MEDIA_ROOT, settings.MEDIA_EXPORT_SUBDIRECTORY)


class FloodfillTaskAPI(APIView):
    @method_decorator(requires_user_role(UserRole.QueueComputeTask))
    def put(self, request, project_id):
        """
        Flood fill a skeleton.
        Files:
        job_config.json
        volume.toml
        diluvian_config.toml
        skeleton.csv
        """

        files = {f.name: f.read().decode("utf-8") for f in request.FILES.values()}
        for x in [
            "job_config.json",
            "volume.toml",
            "diluvian_config.toml",
            "skeleton.csv",
        ]:
            if x not in files.keys():
                Exception(x + " is missing!")
        # pop the job_config since that is needed here, the rest of the files
        # will get passed through to diluvian
        job_config = json.loads(files.pop("job_config.json"))

        # the name of the job, used for storing temporary files
        # and refering to past runs
        job_name = self._get_job_name(job_config)

        # If the temporary directory doesn't exist, create it
        media_folder = Path(settings.MEDIA_ROOT)
        if not (media_folder / job_name).exists():
            (media_folder / job_name).mkdir()
        local_temp_dir = media_folder / job_name

        # Create a copy of the files sent in the request in the
        # temporary directory so that it can be copied with scp
        # in the async function
        for f in files:
            file_path = local_temp_dir / f
            file_path.write_text(files[f])

        # Get the ssh key
        # TODO: determine the most suitable location for ssh keys/paths
        ssh_key_path = settings.SSH_KEY_PATH

        # create a floodfill config object if necessary and pass
        # it to the floodfill results object
        diluvian_config = self._get_diluvian_config(
            request.user.id, project_id, files["diluvian_config.toml"]
        )
        diluvian_config.save()

        # retrieve necessary paths from the chosen server and model
        server = ComputeServer.objects.get(id=job_config["server_id"])
        model = FloodfillModel.objects.get(id=job_config["model_id"])
        server_paths = {
            "address": server.address,
            "diluvian_path": server.diluvian_path,
            "results_dir": server.results_directory,
            "env_source": server.environment_source_path,
            "model_file": model.model_source_path,
        }

        # store a job in the database now so that information about
        # ongoing jobs can be retrieved.
        gpus = self._get_gpus(job_config)
        print(gpus)
        if self._check_gpu_conflict(gpus):
            raise Exception("Not enough compute resources for this job")
        result = FloodfillResult(
            user_id=request.user.id,
            project_id=project_id,
            config_id=diluvian_config.id,
            skeleton_id=job_config["skeleton_id"],
            skeleton_csv=files["skeleton.csv"],
            model_id=job_config["model_id"],
            name=job_name,
            status="queued",
            gpus=gpus,
        )
        result.save()

        # retrieve the configurations used during the chosen models training.
        # this is used as the base configuration when running since most
        # settings should not be changed or are irrelevant to floodfilling a
        # skeleton. The settings that do need to be overridden are handled
        # by the config generated by the widget.
        if model.config_id is not None:
            query = FloodfillConfig.objects.get(id=int(model.config_id))
            model_config = query.config
            file_path = local_temp_dir / "model_config.toml"
            file_path.write_text(model_config)

        msg_user(request.user.id, "floodfilling-result-update", {"status": "queued"})

        if self._check_gpu_conflict():
            raise Exception("Not enough compute resources for this job")
        # Flood filling async function
        x = flood_fill_async.delay(
            result,
            project_id,
            request.user.id,
            ssh_key_path,
            local_temp_dir,
            server_paths,
            job_name,
        )

        # Send a response to let the user know the async funcion has started
        return JsonResponse({"task_id": x.task_id, "status": "queued"})

    def get(self, request, project_id):

        msg_user(request.user.id, "floodfilling-result-update", {"status": "queued"})

        return JsonResponse({"stop": "testing"})

    def _get_job_name(self, config):
        """
        Get the name of a job. If the job_name field is not provided generate a default
        job name based on the date and the skeleton id.
        """
        name = config.get("job_name", "")
        if len(name) == 0:
            skid = str(config.get("skeleton_id", None))
            date = str(datetime.datetime.now().date())
            if skid is None:
                Exception("missing skeleton id!")
            return skid + "_" + date

        return name

    def _get_gpus(self, config):
        gpus = GPUUtilAPI._query_server(config["server_id"])
        config_gpus = config.get("gpus", [])
        if len(config_gpus) == 0:
            config_gpus = list(range(len(gpus)))
        usage = [True if (i in config_gpus) else False for i in range(len(gpus))]
        return usage

    def _check_gpu_conflict(self, gpus=None):
        # returns True if there is a conflict
        ongoing_jobs = FloodfillResult.objects.filter(status="queued")
        if len(ongoing_jobs) == 0:
            # jobs will not have taken compute resources if there
            # are no other jobs. We should probably still check gpu
            # usage stats to see if the gpus are unavailable for some
            # reason other than flood filling jobs.
            return False
        gpu_utils = [job.gpus for job in ongoing_jobs]
        if gpus is not None:
            gpu_utils.append(gpus)
        return (
            len(list(filter(lambda x: x > 1, map(lambda *x: sum(x), *gpu_utils)))) > 1
        )

    def _get_diluvian_config(self, user_id, project_id, config):
        """
        get a configuration object for this project. It may make sense to build more
        robust support for reusing configuration files for running diluvian since
        there are only a couple options and will probably be very similar if not identical
        through many different runs.
        """
        return FloodfillConfig(user_id=user_id, project_id=project_id, config=config)


def importFloodFilledVolume(project_id, user_id, ff_output_file):
    x = np.load(ff_output_file)
    verts = x[0]
    faces = x[1]
    verts = [[v[i] for i in range(len(v))] for v in verts]
    faces = [list(f) for f in faces]

    x = [verts, faces]

    options = {"type": "trimesh", "mesh": x, "title": "skeleton_test"}
    volume = get_volume_instance(project_id, user_id, options)
    volume.save()

    return JsonResponse({"success": True})


@task()
def flood_fill_async(
    result, project_id, user_id, ssh_key_path, local_temp_dir, server, job_name
):
    return time.sleep(60)
    # copy temp files from django local temp media storage to server temp storage
    setup = "scp -i {ssh_key_path} -pr {local_dir} {server_address}:{server_diluvian_dir}/{server_results_dir}/{job_dir}".format(
        **{
            "local_dir": local_temp_dir,
            "server_address": server["address"],
            "server_diluvian_dir": server["diluvian_path"],
            "server_results_dir": server["results_dir"],
            "job_dir": job_name,
            "ssh_key_path": ssh_key_path,
        }
    )
    files = {}
    for f in local_temp_dir.iterdir():
        files[f.name.split(".")[0]] = Path(
            "~/", server["diluvian_path"], server["results_dir"], job_name, f.name
        )

    # connect to the server and run the floodfilling algorithm on the provided skeleton
    flood_fill = """
    ssh -i {ssh_key_path} {server}
    source {server_ff_env_path}
    cd  {server_diluvian_dir}
    python -m diluvian skeleton-fill-parallel {skeleton_file} -s {output_file} -m {model_file} -c {model_config_file} -c {job_config_file} -v {volume_file} --no-in-memory -l WARNING --max-moves 3
    """.format(
        **{
            "ssh_key_path": ssh_key_path,
            "server": server["address"],
            "server_ff_env_path": server["env_source"],
            "server_diluvian_dir": server["diluvian_path"],
            "model_file": server["model_file"],
            "output_file": Path(server["results_dir"], job_name, job_name + "_output"),
            "skeleton_file": files["skeleton"],
            "job_config_file": files["diluvian_config"],
            "model_config_file": files["model_config"],
            "volume_file": files["volume"],
        }
    )

    # Copy the numpy file containing the volume mesh and the csv containing the node connections
    # predicted by the floodfilling run.
    cleanup = """
    scp -i {ssh_key_path} {server}:{server_diluvian_dir}/{server_results_dir}/{server_job_dir}/{output_file_name}.npy {local_temp_dir}
    scp -i {ssh_key_path} {server}:{server_diluvian_dir}/{server_results_dir}/{server_job_dir}/{output_file_name}.csv {local_temp_dir}
    ssh -i {ssh_key_path} {server}
    rm -r {server_diluvian_dir}/{server_results_dir}/{server_job_dir}
    """.format(
        **{
            "ssh_key_path": ssh_key_path,
            "server": server["address"],
            "server_diluvian_dir": server["diluvian_path"],
            "server_results_dir": server["results_dir"],
            "server_job_dir": job_name,
            "output_file_name": job_name + "_output",
            "local_temp_dir": local_temp_dir,
        }
    )

    process = subprocess.Popen(
        "/bin/bash", stdin=subprocess.PIPE, stdout=subprocess.PIPE, encoding="utf8"
    )
    out, err = process.communicate(setup)
    print(out)

    process = subprocess.Popen(
        "/bin/bash", stdin=subprocess.PIPE, stdout=subprocess.PIPE, encoding="utf8"
    )
    out, err = process.communicate(flood_fill)
    print(out)

    process = subprocess.Popen(
        "/bin/bash", stdin=subprocess.PIPE, stdout=subprocess.PIPE, encoding="utf8"
    )
    out, err = process.communicate(cleanup)
    print(out)

    # actually import the volume into the database
    if False:
        importFloodFilledVolume(
            project_id,
            user_id,
            "{}/{}.npy".format(local_temp_dir, job_name + "_output"),
        )

    if False:
        msg = Message()
        msg.user = User.objects.get(pk=int(user_id))
        msg.read = False

        msg.title = "ASYNC JOB MESSAGE HERE"
        msg.text = "IM DOING SOME STUFF, CHECK IT OUT"
        msg.action = "localhost:8000"

        notify_user(user_id, msg.id, msg.title)

    result.completion_time = datetime.datetime.now()
    with open("{}/{}.csv".format(local_temp_dir, job_name + "_output")) as f:
        result.data = f.read()
    result.status = "complete"
    result.save()

    return "complete"


class FloodfillResultAPI(APIView):
    @method_decorator(requires_user_role(UserRole.Browse))
    def get(self, request, project_id):
        """
        List all available floodfilling models
        ---
        parameters:
          - name: project_id
            description: Project of the returned configurations
            type: integer
            paramType: path
            required: true
          - name: model_id
            description: If available, return only the model associated with model_id
            type: int
            paramType: form
            required: false
            defaultValue: false
        """
        result_id = request.query_params.get("result_id", None)
        result = self.get_results(result_id)

        return JsonResponse(
            result, safe=False, json_dumps_params={"sort_keys": True, "indent": 4}
        )

    @method_decorator(requires_user_role(UserRole.QueueComputeTask))
    def delete(self, request, project_id):
        # can_edit_or_fail(request.user, point_id, "point")
        result_id = request.query_params.get("result_id", None)
        if result_id is not None:
            result = get_object_or_404(FloodfillResult, id=result_id)
            result.delete()
            return JsonResponse({"success": True})
        return JsonResponse({"success": False})

    def get_results(self, result_id=None):
        cursor = connection.cursor()
        if result_id is not None:
            cursor.execute(
                """
                SELECT * FROM floodfill_result
                WHERE id = {}
                """.format(
                    result_id
                )
            )
        else:
            cursor.execute(
                """
                SELECT * FROM floodfill_result
                """
            )
        results = dictfetchall(cursor)
        return results


def dictfetchall(cursor):
    "Returns all rows from a cursor as a dict"
    desc = cursor.description
    return [dict(zip([col[0] for col in desc], row)) for row in cursor.fetchall()]


# HELPER FUNCTIONS
def _DTW(seq_a, seq_b):
    """
    Dynamic Time Warping:
    Common algorithm for comparing the similarity of two time series
    while allowing for some compression/stretching along the time axis.

    In this case it might make sense for comparing two traces of a neuron
    since changes in the frequency of node placement should not have a
    large effect on the similarity of the skeleton as a whole.

    assume seq_a and seq_b are of equal length
        (interpolate values before passing to this function)
    """
    Exception("not yet implemented")
    assert len(seq_a) == len(seq_b)
    empty = np.zeros([len(seq_a), len(seq_b)])
    # TODO
    return empty


def _center_of_mass(nodes, data):
    """
    I think we could use center of mass of the floodfilling output segmentations
    to detect missing branches. Assuming floodfilling would detect a large number
    of voxels at the base of a missing branch, this should throw off the center
    of mass of a sliding window following the neurons skeleton. Assuming the
    base of the branch is relatively small, sudden spikes and subsequent drops
    back to the original pattern should indicate the presence of a missing branch
    without the need to excessively expore large regions around the skeleton.

    inputs:
        - sequence of nodes (id, pid, x, y, z)
        - query-able binary volume
    output:
        - sequence of nodes (id, theta, tau)
        - theta is measured on the plane (alpha) perpendicular to the skeleton
        - 0 degrees is the line generated from the intersection of alpha and the xy
            plane or (1,0) centered at the node.
        - tau is the magnitude of the vector from the node to the localized center of mass
    """
    Exception("not yet implemented")

    for node in nodes:
        arc_tangent_center, arc_tangent_slope = _get_tangent(node, nodes)
        volume = _get_data_around_tangent(arc_tangent_center, arc_tangent_slope)
        center_of_mass_vector = _calculate_center(
            volume, arc_tangent_center, arc_tangent_slope
        )
        return (node, center_of_mass_vector)

